{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882567f8-ae37-4d70-82cb-184f714c3c03",
   "metadata": {},
   "source": [
    "# Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
    "categorical features. You have identified that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing valuesD\n",
    "Design a pipeline that includes the following steps\"\n",
    "Use an automated feature selection method to identify the important features in the datasetC\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "Impute the missing values in the numerical columns using the mean of the column valuesC\n",
    "Scale the numerical columns using standardisationC\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "Impute the missing values in the categorical columns using the most frequent value of the columnC\n",
    "One-hot encode the categorical columnsC\n",
    "Combine the numerical and categorical pipelines us#ng a ColumnTransformerC\n",
    "Use a Random Forest Classifier to build the final modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "the pipelineD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a34ed5-26f2-4005-8b89-4836deea4971",
   "metadata": {},
   "source": [
    "## \n",
    "We will use Python and some popular libraries like scikit-learn to implement the pipeline. Let's go through each step:\n",
    "\n",
    "Step 1: Automated Feature Selection\n",
    "For automated feature selection, we can use techniques like Recursive Feature Elimination (RFE) or SelectKBest. Here, we'll use SelectKBest with a chi-squared score to select the top 'k' most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f9108-1693-4862-aa05-df2c3bdb6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def feature_selection(X, y, k=10):\n",
    "    selector = SelectKBest(score_func=chi2, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    return X_selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e925fe-ccc3-4477-8d71-e24c43196550",
   "metadata": {},
   "source": [
    "## Step 2: Numerical Pipeline\n",
    "For numerical columns, we'll handle missing values by imputing them with the mean of the column values and then scale the numerical features using standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f3571e-3b18-408e-8561-652a78fba49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608eb2a-7b54-4d92-baf8-3858ebf69571",
   "metadata": {},
   "source": [
    "## Step 3: Categorical Pipeline\n",
    "For categorical columns, we'll handle missing values by imputing them with the most frequent value of the column and then one-hot encode the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b9b6c3-27b0-416f-a64e-33b260b3c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be3a55-309a-4015-8e83-fdf148eadb7f",
   "metadata": {},
   "source": [
    "## Step 4: Combine Numerical and Categorical Pipelines\n",
    "We'll use ColumnTransformer to combine the numerical and categorical pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0bee3d-727c-4bb5-b4ad-b66c48c78f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Assuming you have a list of column names for numerical and categorical features\n",
    "numerical_features = [...] # List of numerical column names\n",
    "categorical_features = [...] # List of categorical column names\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20443b41-30e1-4911-9764-9e7b0b679b86",
   "metadata": {},
   "source": [
    "## Step 5: Random Forest Classifier\n",
    "Finally, we'll use a Random Forest Classifier to build the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af930c81-5577-4649-8408-87e903acb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming you have the training and test data (X_train, X_test, y_train, y_test)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Combine preprocessor and classifier in a single pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750c612-db22-412c-a550-4884f65515dc",
   "metadata": {},
   "source": [
    "## Interpretation and Possible Improvements:\n",
    "\n",
    "The pipeline we designed allows us to handle missing values, perform feature scaling, and one-hot encoding automatically, making the process efficient and reproducible.\n",
    "The accuracy of the model on the test dataset will give us an idea of how well the pipeline performs.\n",
    "Possible improvements could include trying different feature selection methods, experimenting with hyperparameter tuning for the Random Forest Classifier, and exploring other classification algorithms to see if they yield better results for your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322ce99-b2a2-4e5a-ba5a-31269a9be3d0",
   "metadata": {},
   "source": [
    "# Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Tra#n the pipeline on the iris dataset and evaluate its\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7850724-03ea-4c20-815f-3bb39914efdb",
   "metadata": {},
   "source": [
    "## \n",
    "to build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions. We'll use the famous Iris dataset for this example. Let's go through the steps:\n",
    "\n",
    "Step 1: Import necessary libraries and load the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1affaf8-a26e-407e-9ebf-5c3b8b66cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfde871-0b45-4843-9e38-587497bc5294",
   "metadata": {},
   "source": [
    "##\n",
    "Step 2: Build the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c29eb-dc13-4ec3-bc2b-d008c080798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# StandardScaler for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('ensemble', VotingClassifier(estimators=[('rf', rf_classifier), ('lr', lr_classifier)], voting='hard'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec7366-cda2-45a0-a867-5ea8eef1ea01",
   "metadata": {},
   "source": [
    "##\n",
    "Step 3: Train the pipeline on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e244329-6843-457b-b327-68fb2c1ece90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8ec15-f118-47d6-96d4-30aa4092815b",
   "metadata": {},
   "source": [
    "## \n",
    "Step 4: Evaluate the accuracy of the pipeline on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9ca25-5a20-4800-ab92-e49d570f3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b4db0-3016-4eab-aa20-09a208a3fb8f",
   "metadata": {},
   "source": [
    "##\n",
    "This pipeline combines the predictions of both the Random Forest Classifier and the Logistic Regression Classifier using a Voting Classifier with a 'hard' voting strategy. The 'hard' voting strategy takes the majority vote of the classifiers, meaning the final prediction is the class that receives the most votes from the individual classifiers.\n",
    "\n",
    "By running this code, you should get the accuracy of the pipeline on the Iris test dataset. The accuracy will give you an idea of how well the combined model performs on the given dataset.\n",
    "\n",
    "Remember to tune hyperparameters and try different classifiers to potentially improve the performance of the voting classifier on the Iris dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
